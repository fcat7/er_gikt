"""
è®­ç»ƒå¹¶æµ‹è¯•æ¨¡åž‹
ä½¿ç”¨äº”æŠ˜äº¤å‰éªŒè¯æ³•
"""
import os
import time
from datetime import datetime
import numpy as np
from scipy import sparse
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import KFold, ShuffleSplit
from torch.utils.data import DataLoader, Subset
from dataset import UserDataset
from config import Config, DEVICE, COLOR_LOG_B, COLOR_LOG_Y, COLOR_LOG_G, COLOR_LOG_END
from params import HyperParameters
from util.utils import gen_gikt_graph, build_adj_list
import argparse
from gikt import GIKT

# @add_fzq: AMP Support
import torch
from torch.cuda.amp import autocast, GradScaler

# è‡ªåŠ¨æ£€æµ‹ AMP å¯ç”¨æ€§
# P100 (Pascal) ä¸æ”¯æŒ Tensor Core åŠ é€Ÿï¼Œä½† fp16 å¯èŠ‚çœæ˜¾å­˜
# RTX 4050 (Ada) æ”¯æŒ Tensor Coreï¼ŒåŠ é€Ÿæ˜Žæ˜¾
use_amp = torch.cuda.is_available() # æš‚æ—¶å…¨å¼€ï¼Œç”± autocast å†…éƒ¨å¤„ç†ç¡¬ä»¶å…¼å®¹
scaler = GradScaler(enabled=use_amp)
if use_amp:
    print(f"ðŸš€ AMP Enabled on {torch.cuda.get_device_name(0)}")
else:
    print(f"âš ï¸ AMP Disabled")

# try :
#     from icecream import ic
#     print = ic
# except ImportError:
#     print("warning: icecream not installed, using standard print.")


# @add_fzq 2025-12-25 10:42:10 -------------------------------------------
# å›ºå®šéšæœºç§å­ï¼Œä¿è¯å®žéªŒå¯å¤çŽ°
import random
import torch
def set_seed(seed=42):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(42)
# @add_fzq 2025-12-25 10:42:10 -------------------------------------------

def get_parser():
    parser = argparse.ArgumentParser(description="Train and Test GIKT Model")
    parser.add_argument('--full', action='store_true', help='Use full dataset (overrides --mode)')

    parser.add_argument('--name', type=str, default='default', help='Name of the experiment')
    return parser

def get_exp_config_path(isFull=False, name='default'):
    # é»˜è®¤è·¯å¾„ä¸ºï¼š config/experiments/exp_gcn_sample_default.toml
    return f"config/experiments/exp_{'full' if isFull else 'sample'}_{name}.toml"

# ä½¿ç”¨æ–¹æ³•
# Windows PowerShell ç¦ç”¨ GPU
# $env:CUDA_VISIBLE_DEVICES = "-1"  
# python train_test.py 
# python train_test.py --full
# python train_test.py --agg_method gat
# python train_test.py --name my_exp
# python train_test.py --name mc_optim_a
if __name__ == '__main__':

    parser = get_parser()
    args = parser.parse_args()

    # os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # âš ï¸ ä»…ç”¨äºŽè°ƒè¯•ï¼Œä¼šå¤§å¹…é™ä½Žæ€§èƒ½ï¼æ­£å¸¸è®­ç»ƒæ—¶åº”æ³¨é‡ŠæŽ‰

    # @add_fzq 2025-12-24 17:28:09 -------------------------------------------
    # 1. è§£å†³æ—¶åŒºé—®é¢˜ï¼šå¼ºåˆ¶ä½¿ç”¨ UTC+8 (åŒ—äº¬æ—¶é—´)
    from datetime import timedelta, timezone
    beijing_time = datetime.now(timezone(timedelta(hours=8)))
    time_now = beijing_time.strftime('%Y%m%d_%H%M')
    # @add_fzq 2025-12-24 17:28:09 -------------------------------------------

    # åŠ è½½è¶…å‚æ•°
    exp_config_path = get_exp_config_path(isFull=args.full, name=args.name)
    params = HyperParameters.load(exp_config_path=exp_config_path)
    # åŠ è½½é…ç½®
    dataset_name = params.train.dataset_name
    config = Config(dataset_name=dataset_name)

    output_path = f'{config.path.LOG_DIR}/{time_now}.log'
    output_dir = os.path.dirname(output_path)  # èŽ·å–ç›®å½•è·¯å¾„    
    os.makedirs(output_dir, exist_ok=True) # åˆ›å»ºç›®å½•ï¼ˆå¦‚æžœä¸å­˜åœ¨ï¼‰
    output_file = open(output_path, 'a', buffering=1) # è§£å†³æ—¥å¿—ä¸¢å¤±é—®é¢˜ï¼Œä½¿ç”¨ 'a' (append) æ¨¡å¼ï¼Œå¹¶è®¾ç½® buffering=1 (è¡Œç¼“å†²)

    print(f"Using dataset: {dataset_name}, Data dir: {config.PROCESSED_DATA_DIR}\n")
    print(f"Using experiment config: {exp_config_path}\n")
    
    # æ‰“å°å½“å‰ä½¿ç”¨çš„è®¾å¤‡çš„åç§° get_device_name
    print(f"Using device: {torch.cuda.is_available() and torch.cuda.get_device_name(0) or 'cpu'}\n")
    

    # æ‰“å°å¹¶å†™è¶…å‚æ•°
    output_file.write(str(params) + '\n')
    print(params)
    
    batch_size = params.train.batch_size
    
    # æž„å»ºæ¨¡åž‹éœ€è¦çš„æ•°æ®ç»“æž„, å…¨éƒ¨è½¬åŒ–ä¸ºæ­£ç¡®ç±»åž‹tensorå†è¾“å…¥æ¨¡åž‹ä¸­
    qs_table = torch.tensor(sparse.load_npz(os.path.join(config.PROCESSED_DATA_DIR, 'qs_table.npz')).toarray(), dtype=torch.int64, device=DEVICE)  # [num_q, num_c]
    num_question = torch.tensor(qs_table.shape[0], device=DEVICE)
    num_skill = torch.tensor(qs_table.shape[1], device=DEVICE)
    q_neighbors_list, s_neighbors_list = build_adj_list(config.PROCESSED_DATA_DIR)
    
    # Config ä½¿ç”¨ç»“æž„åŒ–è®¿é—®
    q_neighbors, s_neighbors = gen_gikt_graph(q_neighbors_list, s_neighbors_list, params.model.size_q_neighbors, params.model.size_s_neighbors)
    q_neighbors = torch.tensor(q_neighbors, dtype=torch.int64, device=DEVICE)
    s_neighbors = torch.tensor(s_neighbors, dtype=torch.int64, device=DEVICE)

    # å¤„ç† dropout (list -> tuple)
    dropout_val = params.model.dropout
    if isinstance(dropout_val, list):
        dropout_val = tuple(dropout_val)

    # åˆå§‹åŒ–æ¨¡åž‹
    model = GIKT(
        num_question, num_skill, q_neighbors, s_neighbors, qs_table,
        agg_hops=params.model.agg_hops,
        emb_dim=params.model.emb_dim,
        dropout=dropout_val,
        hard_recap=params.model.hard_recap,
        use_cognitive_model=params.model.use_cognitive_model,
        pre_train=params.model.pre_train,
        data_dir=config.PROCESSED_DATA_DIR,
        agg_method=params.model.agg_method,
        recap_source='hsei' if params.model.use_input_attention else 'hssi', # é€šè¿‡ toml é…ç½®æŽ§åˆ¶
        enable_tf_alignment=params.model.enable_tf_alignment
    ).to(DEVICE)

    # @change_fzq 2026-01-08: ä¿®æ”¹æŸå¤±å‡½æ•°ä¸º BCELoss
    # åŽŸå› ï¼šæ¨¡åž‹è¾“å‡ºå·²ç»æ˜¯ Sigmoid æ¦‚çŽ‡ï¼ŒBCEWithLogitsLoss ä¼šå†æ¬¡ Sigmoidï¼Œå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±
    # åŽŸå§‹æŸå¤±å‡½æ•°å¤‡ä»½ï¼š
    loss_fun = torch.nn.BCEWithLogitsLoss().to(DEVICE) # æŸå¤±å‡½æ•°
    if params.train.use_bce_loss:
        loss_fun = torch.nn.BCELoss().to(DEVICE)

    # @add_fzq: TF Alignment Override
    if params.model.enable_tf_alignment:
        # If alignment enabled, Model outputs Logits -> Must use BCEWithLogitsLoss
        loss_fun = torch.nn.BCEWithLogitsLoss().to(DEVICE)
    
    dataset = UserDataset(config)  # æ•°æ®é›†
    data_len = len(dataset)  # æ•°æ®æ€»é•¿åº¦

    # å†™å½“å‰æ•°æ®é‡
    output_file.write(f'Total number of users in dataset: {data_len}\n')
    print('model has been built')

    # @add_fzq 2025-12-24 17:28:09 -------------------------------------------
    # è®°å½•æ€»å¼€å§‹æ—¶é—´
    total_start_time = time.time()
    # @add_fzq 2025-12-24 17:28:09 -------------------------------------------

    # ä¼˜åŒ–å™¨
    epoch_total = 0
    optimizer = torch.optim.Adam(params=model.parameters(), lr=params.train.lr)
    torch.optim.lr_scheduler.ExponentialLR(optimizer, params.train.lr_gamma)
    
    # åœ¨matplotlibä¸­ç»˜åˆ¶çš„yè½´æ•°æ®ï¼Œä¸‰è¡Œåˆ†åˆ«è¡¨ç¤ºloss, acc, auc
    y_label_aver = np.zeros([3, params.train.epochs]) # å¹³å‡ç²¾åº¦å€¼
    y_label_all = np.zeros([3, params.train.epochs * params.train.k_fold]) # å…¨éƒ¨ç²¾åº¦å€¼

    # KFoldçš„shuffleæ“ä½œæ˜¯åœ¨ç”¨æˆ·çº§åˆ«è¿›è¡Œçš„ï¼Œè€Œä¸æ˜¯åœ¨ç­”é¢˜è®°å½•çº§åˆ«
    if params.train.k_fold == 1:
        # å¦‚æžœ k_fold ä¸º 1ï¼Œä½¿ç”¨ ShuffleSplit è¿›è¡Œå•æ¬¡åˆ’åˆ† (80% è®­ç»ƒ, 20% æµ‹è¯•)
        k_fold = ShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
    else:
        # å¦åˆ™ä½¿ç”¨ KFold è¿›è¡Œäº¤å‰éªŒè¯
        k_fold = KFold(n_splits=params.train.k_fold, shuffle=True, random_state=42)

    for epoch in range(params.train.epochs):
        train_loss_aver = train_acc_aver = train_auc_aver = 0
        test_loss_aver = test_acc_aver = test_auc_aver = 0
        # äº”æŠ˜çš„å¹³å‡å€¼
        for fold, (train_indices, test_indices) in enumerate(k_fold.split(dataset)):
            # ä½¿ç”¨äº”æŠ˜äº¤å‰éªŒè¯ï¼Œæ¯æ¬¡çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†éƒ½ä¸ç›¸åŒ
            train_set = Subset(dataset, train_indices)  # è®­ç»ƒé›†
            test_set = Subset(dataset, test_indices)  # æµ‹è¯•é›†
            if DEVICE.type == 'cpu':  # Cpu(æœ¬æœº)
                train_loader = DataLoader(train_set, batch_size=batch_size)  # è®­ç»ƒæ•°æ®åŠ è½½å™¨
                test_loader = DataLoader(test_set, batch_size=batch_size)  # æµ‹è¯•æ•°æ®åŠ è½½å™¨
            else:  # Gpu(æœåŠ¡å™¨)
                # @fix_fzq: prefetch_factor only works with num_workers > 0
                loader_kwargs = {
                    'batch_size': batch_size,
                    'num_workers': params.common.num_workers,
                    'pin_memory': True
                }
                if params.common.num_workers > 0:
                    loader_kwargs['prefetch_factor'] = params.train.prefetch_factor
                
                train_loader = DataLoader(train_set, **loader_kwargs)
                test_loader = DataLoader(test_set, **loader_kwargs)
            train_data_len, test_data_len = len(train_set), len(test_set)
            # @delete_fzq 2025-12-23 22:10:41
            #  print('===================' + COLOR_LOG_Y + f'epoch: {epoch_total + 1}'+ COLOR_LOG_END + '====================')
            # @add_fzq 2025-12-23 22:10:41
            print('===================' + COLOR_LOG_Y + f'fold: {fold + 1}'+ COLOR_LOG_END + '====================')

            # è®­ç»ƒé˜¶æ®µï¼Œæ—¢æœ‰å‰å‘ä¼ æ’­ï¼Œä¹Ÿæœ‰åå‘ä¼ æ’­
            print('-------------------training------------------')
            torch.set_grad_enabled(True) # @add_fzq: Enable grad for training
            model.train() # @add_fzq: Switch to train mode
            time0 = time.time()
            train_step = train_loss = train_total = train_right = train_auc = 0
            # æ¯è½®è®­ç»ƒç¬¬å‡ ä¸ªæ‰¹é‡, æ€»æŸå¤±, è®­ç»ƒçš„çœŸå®žæ ·æœ¬ä¸ªæ•°, å…¶ä¸­æ­£ç¡®çš„ä¸ªæ•°, æ€»ä½“è®­ç»ƒçš„auc
            for data in train_loader:
                # æ¢¯åº¦æ¸…é›¶
                optimizer.zero_grad()

                # -- delete_fzq 2025-12-25 17:15:13-----------------
                # x, y_target, mask = data[:, :, 0].to(DEVICE), data[:, :, 1].to(DEVICE), data[:, :, 2].to(torch.bool).to(DEVICE)
                # y_hat = model(x, y_target, mask) # åŽŸå§‹ä»£ç 
                # --------------------------------------------
                # æ‰¹é‡è½¬ç§»åˆ° GPUï¼ˆå•æ¬¡ PCIeï¼‰
                data_gpu = data.to(DEVICE)
                # åœ¨ GPU ä¸Šåˆ‡ç‰‡å’Œç±»åž‹è½¬æ¢ï¼ˆGPU å†…æ“ä½œï¼Œæ— å¼€é”€ï¼‰
                x = data_gpu[:, :, 0].to(torch.long)
                y_target = data_gpu[:, :, 1].to(torch.long)
                mask = data_gpu[:, :, 2].to(torch.bool)
                interval_time = data_gpu[:, :, 3].to(torch.float32)
                response_time = data_gpu[:, :, 4].to(torch.float32)

                # @delete_fzq 2026-02-04: ç§»é™¤è®­ç»ƒå¾ªçŽ¯å†…çš„ NaN æ£€æŸ¥ä»¥åŠ é€Ÿ
                # if torch.isnan(interval_time).any():
                #    interval_time = torch.nan_to_num(interval_time, nan=0.0)
                # if torch.isnan(response_time).any():
                #    response_time = torch.nan_to_num(response_time, nan=0.0)

                with autocast(enabled=use_amp):
                    y_hat = model(x, y_target, mask, interval_time, response_time)
                    # --------------------------------------------
        
                    y_hat = torch.masked_select(y_hat, mask)
                    y_target = torch.masked_select(y_target, mask)

                    # @add_fzq: Logic Branch for TF Alignment (Logits vs Probs)
                    if params.model.enable_tf_alignment:
                        # y_hat is logits. No clamping needed for BCEWithLogitsLoss.
                        loss = loss_fun(y_hat, y_target.to(torch.float32))
                        
                        # Metrics: Convert to Probabilities
                        y_prob = torch.sigmoid(y_hat) 
                    else: 
                        # Original Behavior: y_hat is probabilities (Sigmoid applied in model)
                        # @add_fzq 2026-01-08: Clamping for BCELoss numerical stability
                        y_hat = torch.clamp(y_hat, min=1e-6, max=1.0 - 1e-6)
                        loss = loss_fun(y_hat, y_target.to(torch.float32))
                        
                        y_prob = y_hat # Already probabilities
                    
                    # @add_fzq: Regularization Constraint (Path 2 & 3)
                    # é˜²æ­¢åŒºåˆ†åº¦å‚æ•°çˆ†ç‚¸ã€‚å¯¹äºŽ Step 2 (scalar)ï¼Œçº¦æŸå…¶è¶‹è¿‘ 0(å³gain=1)ï¼›
                    # å¯¹äºŽ Step 3 (Embedding)ï¼Œçº¦æŸæ•´ä¸ªè¡¨ã€‚
                    reg_loss = 0.0
                    if hasattr(model, 'discrimination_gain'):
                        reg_loss += 0.01 * (model.discrimination_gain ** 2)
                    if hasattr(model, 'discrimination_bias'):
                        # åŒºåˆ†åº¦æ­£åˆ™ï¼šé¼“åŠ±å…¶é è¿‘ 1.0 (å³åå·®é è¿‘ 0)
                        reg_loss += 1e-5 * torch.sum(model.discrimination_bias.weight ** 2)
                    
                    if hasattr(model, 'guessing_bias') and hasattr(model, 'slipping_bias'):
                        # çŒœæµ‹å’Œå¤±è¯¯çŽ‡æ­£åˆ™ï¼šé˜²æ­¢å®ƒä»¬è¿‡å¤§
                        # å› ä¸º sigmoid(-3) çº¦ç­‰äºŽ 0.05ï¼Œæˆ‘ä»¬ä¸å¸Œæœ›è¿™äº›å‚æ•°æ¼‚ç§»å›ž 0 (0.5) æˆ–æ›´é«˜
                        # è¿™é‡Œé™åˆ¶å…¶æƒé‡çš„ L2ï¼Œä½†æ›´é‡è¦çš„æ˜¯é™åˆ¶å…¶ä¸è¦å˜å¾—å¤ªå¤§
                        reg_loss += 1e-5 * torch.sum(torch.relu(model.guessing_bias.weight + 2.0)**2) 
                        reg_loss += 1e-5 * torch.sum(torch.relu(model.slipping_bias.weight + 3.0)**2)
                    
                    loss += reg_loss
                
                # @add_fzq: AMP Backward
                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()
                
                train_loss += loss.item()
                
                # è®¡ç®—acc
                y_pred = torch.ge(y_prob, torch.tensor(0.5))
                acc = torch.sum(torch.eq(y_target, y_pred)) / torch.sum(mask)
                train_right += torch.sum(torch.eq(y_target, y_pred))
                train_total += torch.sum(mask)
                # è®¡ç®—auc
                # @optimize: Use probabilities (y_prob) instead of labels (y_pred) for better precision
                auc = roc_auc_score(y_target.cpu().detach(), y_prob.cpu().detach())
                train_auc += auc * len(x) / train_data_len
                
                train_step += 1
                if params.train.verbose:
                    print(f'step: {train_step}, loss: {loss.item():.4f}, acc: {acc.item():.4f}, auc: {auc:.4f}')
            train_loss, train_acc = train_loss / train_step, train_right / train_total
            train_loss_aver += train_loss
            train_acc_aver += train_acc
            train_auc_aver += train_auc

            # æµ‹è¯•é˜¶æ®µï¼Œåªæœ‰å‰å‘ä¼ é€’ï¼Œæ²¡æœ‰åå‘ä¼ æ’­é˜¶æ®µ
            print('-------------------testing------------------')
            model.eval() # @add_fzq: Switch to eval mode
            test_step = test_loss = test_total = test_right = test_auc = 0
            
            # @add_fzq: Global AUC Support
            all_y_targets = []
            all_y_probs = []

            # æ¯è½®è®­ç»ƒç¬¬å‡ ä¸ªæ‰¹é‡, æ€»æŸå¤±, è®­ç»ƒçš„çœŸå®žæ ·æœ¬ä¸ªæ•°, å…¶ä¸­æ­£ç¡®çš„ä¸ªæ•°, æ€»ä½“çš„auc
            torch.set_grad_enabled(False) # @add_fzq: Disable grad for testing (Save VRAM)
            for data in test_loader:
                    # -- delete_fzq 2025-12-25 17:15:13-----------------
                    # x, y_target, mask = data[:, :, 0].to(DEVICE), data[:, :, 1].to(DEVICE), data[:, :, 2].to(torch.bool).to(DEVICE)
                    # y_hat = model(x, y_target, mask) # åŽŸå§‹ä»£ç 
                    # --------------------------------------------

                    # ---------------- æ–°å¢žæ—¶é—´ç‰¹å¾ ----------------
                    # -- add_fzq 2025-12-25 17:15:13-----------------
                    # æ‰¹é‡è½¬ç§»åˆ° GPUï¼ˆå•æ¬¡ PCIeï¼‰
                    data_gpu = data.to(DEVICE)

                    # åœ¨ GPU ä¸Šåˆ‡ç‰‡å’Œç±»åž‹è½¬æ¢ï¼ˆGPU å†…æ“ä½œï¼Œæ— å¼€é”€ï¼‰
                    x = data_gpu[:, :, 0].to(torch.long)
                    y_target = data_gpu[:, :, 1].to(torch.long)
                    mask = data_gpu[:, :, 2].to(torch.bool)
                    interval_time = data_gpu[:, :, 3].to(torch.float32)
                    response_time = data_gpu[:, :, 4].to(torch.float32)
                
                    # @delete_fzq 2026-02-04: ç§»é™¤æµ‹è¯•å¾ªçŽ¯å†…çš„ NaN æ£€æŸ¥ä»¥åŠ é€Ÿ
                    # if torch.isnan(interval_time).any():
                    #    interval_time = torch.nan_to_num(interval_time, nan=0.0)
                    # if torch.isnan(response_time).any():
                    #    response_time = torch.nan_to_num(response_time, nan=0.0)

                    with autocast(enabled=use_amp):
                        y_hat = model(x, y_target, mask, interval_time, response_time)
                    # -- add_fzq 2025-12-25 17:15:13-----------------
                    # --------------------------------------------
                
                    y_hat = torch.masked_select(y_hat, mask.to(torch.bool))
                    y_target = torch.masked_select(y_target, mask.to(torch.bool))
                
                    # @add_fzq: Logic Branch for TF Alignment (Testing Phase)
                    if params.model.enable_tf_alignment:
                        loss = loss_fun(y_hat, y_target.to(torch.float32))
                        y_prob = torch.sigmoid(y_hat)
                    else:
                        # @add_fzq 2026-01-08: æˆªæ–­
                        y_hat = torch.clamp(y_hat, min=1e-6, max=1.0 - 1e-6)
                        loss = loss_fun(y_hat, y_target.to(torch.float32))
                        y_prob = y_hat

                    test_loss += loss.item()
                    
                    # è®¡ç®—acc
                    y_pred = torch.ge(y_prob, torch.tensor(0.5))
                    acc = torch.sum(torch.eq(y_target, y_pred)) / torch.sum(mask)
                    test_right += torch.sum(torch.eq(y_target, y_pred))
                    test_total += torch.sum(mask)
                    # è®¡ç®—auc
                    if params.train.use_global_auc:
                        all_y_targets.extend(y_target.cpu().detach().numpy())
                        all_y_probs.extend(y_prob.cpu().detach().numpy())
                        test_step += 1
                        if params.train.verbose:
                            try:
                                batch_auc = roc_auc_score(y_target.cpu().detach(), y_prob.cpu().detach())
                                print(f'step: {test_step}, loss: {loss.item():.4f}, acc: {acc.item():.4f}, auc: {batch_auc:.4f} (Batch)')
                            except ValueError:
                                pass
                    else:
                        try:
                            auc = roc_auc_score(y_target.cpu().detach(), y_prob.cpu().detach())
                            test_auc += auc * len(x) / test_data_len
                            test_step += 1
                            if params.train.verbose:
                                print(f'step: {test_step}, loss: {loss.item():.4f}, acc: {acc.item():.4f}, auc: {auc:.4f}')
                        except ValueError:
                            test_step += 1
            
            if params.train.use_global_auc and len(all_y_targets) > 0:
                test_auc = roc_auc_score(all_y_targets, all_y_probs)

            test_loss, test_acc = test_loss / test_step, test_right / test_total
            test_loss_aver += test_loss
            test_acc_aver += test_acc
            test_auc_aver += test_auc

            # foldæ€»ç»“é˜¶æ®µ
        
            time1 = time.time()
            run_time = time1 - time0
            print(COLOR_LOG_B + f'training: loss: {train_loss:.4f}, acc: {train_acc:.4f}, auc: {train_auc: .4f}' + COLOR_LOG_END)
            print(COLOR_LOG_B + f'testing: loss: {test_loss:.4f}, acc: {test_acc:.4f}, auc: {test_auc: .4f}' + COLOR_LOG_END)
            print(COLOR_LOG_B + f'time: {run_time:.2f}s, average batch time: {(run_time / (test_step + train_step)):.2f}s' + COLOR_LOG_END)
            # ä¿å­˜è¾“å‡ºè‡³æœ¬åœ°æ–‡ä»¶
            output_file.write(f'  fold {fold+1} | ')
            output_file.write(f'training: loss: {train_loss:.4f}, acc: {train_acc:.4f}, auc: {train_auc: .4f}\n         | ')
            output_file.write(f'testing: loss: {test_loss:.4f}, acc: {test_acc:.4f}, auc: {test_auc: .4f} | ')
            output_file.write(f'time: {run_time:.2f}s, average batch time: {(run_time / (test_step + train_step)):.2f}s\n')
            # ä¿å­˜è‡³æ•°ç»„ï¼Œä¹‹åŽç”¨matplotlibç”»å›¾
            y_label_all[0][fold], y_label_all[1][fold], y_label_all[2][fold] = test_loss, test_acc, test_auc

        # epochæ€»ç»“é˜¶æ®µ
        epoch_total += 1
        train_loss_aver /= params.train.k_fold
        train_acc_aver /= params.train.k_fold
        train_auc_aver /= params.train.k_fold
        test_loss_aver /= params.train.k_fold
        test_acc_aver /= params.train.k_fold
        test_auc_aver /= params.train.k_fold
        print('>>>>>>>>>>>>>>>>>>' + COLOR_LOG_Y + f"epoch: {epoch_total}"+ COLOR_LOG_END + '<<<<<<<<<<<<<<<<<<')
        print(COLOR_LOG_G + f'training: loss: {train_loss_aver:.4f}, acc: {train_acc_aver:.4f}, auc: {train_auc_aver: .4f}' + COLOR_LOG_END)
        print(COLOR_LOG_G + f'testing: loss: {test_loss_aver:.4f}, acc: {test_acc_aver:.4f}, auc: {test_auc_aver: .4f}' + COLOR_LOG_END)
        output_file.write(f"epoch: {epoch_total} | ")
        output_file.write(f'training: loss: {train_loss_aver:.4f}, acc: {train_acc_aver:.4f}, auc: {train_auc_aver: .4f}\n         | ')
        output_file.write(f'testing: loss: {test_loss_aver:.4f}, acc: {test_acc_aver:.4f}, auc: {test_auc_aver: .4f}\n')
        y_label_aver[0][epoch], y_label_aver[1][epoch], y_label_aver[2][epoch] = test_loss_aver, test_acc_aver, test_auc_aver

        # @add_fzq: å®žæ—¶ä¿å­˜ aver æ•°æ®ï¼Œé˜²æ­¢è®­ç»ƒä¸­æ–­ä¸¢å¤±æ•°æ®
        # æ¯æ¬¡ epoch ç»“æŸéƒ½è¦†ç›–ä¿å­˜ä¸€æ¬¡ï¼Œç¡®ä¿å³ä½¿ä¸­æ–­ä¹Ÿèƒ½ä¿ç•™å·²å®Œæˆçš„ epoch æ•°æ®
        # np.savetxt(f'{LOGCHART_DIR}/{time_now}_aver.txt', y_label_aver)

    # @add_fzq 2025-12-24 17:28:09 -------------------------------------------
    # è®¡ç®—æ€»è€—æ—¶ 
    total_end_time = time.time()
    total_duration = total_end_time - total_start_time
    total_hours = int(total_duration // 3600)
    total_minutes = int((total_duration % 3600) // 60)
    total_seconds = int(total_duration % 60)
    time_str = f'{total_hours}h {total_minutes}min {total_seconds}s'
    print(f'Total training time: {time_str}')
    output_file.write(f'Total training time: {time_str}\n')
    # @add_fzq 2025-12-24 17:28:09 -------------------------------------------

    output_file.close()

    torch.save(model, f=f'{config.path.MODEL_DIR}/{time_now}.pt')
    np.savetxt(f'{config.path.CHART_DIR}/{time_now}_all.txt', y_label_all)
    np.savetxt(f'{config.path.CHART_DIR}/{time_now}_aver.txt', y_label_aver)